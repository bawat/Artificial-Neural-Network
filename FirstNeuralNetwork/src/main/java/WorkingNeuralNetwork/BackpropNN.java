/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package WorkingNeuralNetwork;

import java.text.DecimalFormat;
import java.time.LocalDateTime;
import java.time.temporal.ChronoUnit;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Optional;
import java.util.Random;
import java.util.Set;
import java.util.function.BiFunction;
import java.util.function.Function;
import java.util.function.Supplier;
import java.util.stream.Collectors;
import java.util.stream.Stream;

import WorkingNeuralNetwork.BruteForceNN.NN;
import WorkingNeuralNetwork.BackpropNN.NN.NNParameter;

public class BackpropNN {
	public static void main(String[] args) {
		NN.InputLayerNode 
			n00 = new NN.InputLayerNode(0),
		    n01 = new NN.InputLayerNode(1);
		
		NN.HiddenLayerNode
			n10 = new NN.HiddenLayerNode(1,0),
		    n11 = new NN.HiddenLayerNode(1,1),
		    
			n20 = new NN.HiddenLayerNode(2,0),
		    n21 = new NN.HiddenLayerNode(2,1);
		
		NN.OutputLayerNode 
			n30 = new NN.OutputLayerNode(3,0),
		    n31 = new NN.OutputLayerNode(3,1);
		    
		NN.Weight
			w00to10 = new NN.Weight(n00,n10),
			w01to10 = new NN.Weight(n01,n10),
	
			w00to11 = new NN.Weight(n00,n11),
			w01to11 = new NN.Weight(n01,n11),
			
	
			w10to20 = new NN.Weight(n10,n20),
			w11to20 = new NN.Weight(n11,n20),
	
			w10to21 = new NN.Weight(n10,n21),
			w11to21 = new NN.Weight(n11,n21),
		    		
	
			w20to30 = new NN.Weight(n20,n30),
			w21to30 = new NN.Weight(n21,n30),
	
			w20to31 = new NN.Weight(n20,n31),
			w21to31 = new NN.Weight(n21,n31);
		
		NN.Bias
	    	b10 = new NN.Bias(n10),
		    b11 = new NN.Bias(n11),
		    
			b20 = new NN.Bias(n20),
		    b21 = new NN.Bias(n21),
		    
			b30 = new NN.Bias(n30),
		    b31 = new NN.Bias(n31);
		
		Runnable printNN = () -> {
			DecimalFormat f = new DecimalFormat("+#00.00;#00.00");
			System.out.println("");
    		System.out.println("                  b("+f.format(b10.value)+")        b("+f.format(b20.value)+")        b("+f.format(b30.value)+")");
    		System.out.println("                  |              |              |");
    		System.out.println("X1->("+f.format(0d)+")--"+f.format(w00to10.value)+"-->("+f.format(0d)+")--"+f.format(w10to20.value)+"-->("+f.format(0d)+")--"+f.format(w20to30.value)+"-->("+f.format(0d)+")");
    		System.out.println("          \\      /       \\      /       \\      /");
    		System.out.println("           "+f.format(w00to11.value)+" /         "+f.format(w10to21.value)+" /         "+f.format(w20to31.value)+" /");
    		System.out.println("            \\  /           \\  /           \\  /");
    		System.out.println("             \\/             \\/             \\/");
    		System.out.println("             /\\             /\\             /\\");
    		System.out.println("            /  \\           /  \\           /  \\");
    		System.out.println("           "+f.format(w01to10.value)+" \\         "+f.format(w11to20.value)+" \\         "+f.format(w21to30.value)+" \\");
    		System.out.println("          /      \\       /      \\       /      \\");
    		System.out.println("X2->("+f.format(0d)+")--"+f.format(w01to11.value)+"-->("+f.format(0d)+")--"+f.format(w11to21.value)+"-->("+f.format(0d)+")--"+f.format(w21to31.value)+"-->("+f.format(0d)+")");
    		System.out.println("                  |              |              |");
    		System.out.println("                  b("+f.format(b11.value)+")        b("+f.format(b21.value)+")        b("+f.format(b31.value)+")");
		};
		
		//LeCun Initialisation
		NN.initWeights((fanIn, fanOut) -> new Random().nextGaussian()/((double) fanIn));
		NN.initBiases((fanIn, fanOut) -> new Random().nextGaussian()/((double) fanIn));
		
		//trainOneInput(printNN);
		trainTwoInputs(printNN);
	}
	
    public static void trainOneInput(Runnable printNN) {
    	
    	ArrayList<Double> input1 = new ArrayList<>();
    	input1.add(1d);
    	input1.add(0.5d);
    	
    	ArrayList<Double> expectedOutput1 = new ArrayList<>();
    	expectedOutput1.add(7d);
    	expectedOutput1.add(20d);
    	
    	InputsAndExpectedOutputs trainingData = new InputsAndExpectedOutputs(
    			new InputAndExpectedOutput(input1, expectedOutput1)
    	);
    	
    	LocalDateTime startTime = LocalDateTime.now();
    	gradientDescentTraining(trainingData, printNN);
    	System.out.println("Testing [1.0,0.5] -> [7, 20] actual (" + forwardProp(input1) + ")");
    	System.out.println("Time Taken: " + startTime.until(LocalDateTime.now(), ChronoUnit.SECONDS) + " seconds ");
    }
    
    public static void trainTwoInputs(Runnable printNN) {
    	
    	ArrayList<Double> input1 = new ArrayList<>();
    	input1.add(1d);
    	input1.add(0.5d);
    	
    	ArrayList<Double> expectedOutput1 = new ArrayList<>();
    	expectedOutput1.add(7d);
    	expectedOutput1.add(20d);
    	

    	
    	ArrayList<Double> input2 = new ArrayList<>();
    	input2.add(0.3d);
    	input2.add(0.7d);
    	
    	ArrayList<Double> expectedOutput2 = new ArrayList<>();
    	expectedOutput2.add(101d);
    	expectedOutput2.add(99d);
    	

    	
    	ArrayList<Double> input3 = new ArrayList<>();
    	input3.add(0.1d);
    	input3.add(0.1d);
    	
    	ArrayList<Double> expectedOutput3 = new ArrayList<>();
    	expectedOutput3.add(69d);
    	expectedOutput3.add(69d);
    	
    	
    	InputsAndExpectedOutputs trainingData = new InputsAndExpectedOutputs(
    			new InputAndExpectedOutput(input1, expectedOutput1),
    			new InputAndExpectedOutput(input2, expectedOutput2),
    			new InputAndExpectedOutput(input3, expectedOutput3)
    	);
    	
    	LocalDateTime startTime = LocalDateTime.now();
    	gradientDescentTraining(trainingData, printNN);
    	System.out.println("Testing [1.0,0.5] -> [7, 20] actual (" + forwardProp(input1) + ")");
    	System.out.println("Testing [0.3,0.7] -> [101, 99] actual (" + forwardProp(input2) + ")");
    	System.out.println("Testing [0.1,0.1] -> [69, 69] actual (" + forwardProp(input3) + ")");
    	System.out.println("Time Taken: " + startTime.until(LocalDateTime.now(), ChronoUnit.SECONDS) + " seconds ");
    }
    
    static class InputsAndExpectedOutputs{
    	ArrayList<InputAndExpectedOutput> trainingData = new ArrayList<>();
    	InputsAndExpectedOutputs(InputAndExpectedOutput... trainingData){
    		this.trainingData = new ArrayList<>(Arrays.asList(trainingData));
    	}
    }
    static class InputAndExpectedOutput{
    	ArrayList<Double> inputs;
    	ArrayList<Double> outputs;
    	InputAndExpectedOutput(ArrayList<Double> inputs, ArrayList<Double> outputs){
    		this.inputs = inputs;
    		this.outputs = outputs;
    	}
    }
    
    private static Stream<NN.Node> getNodes(){
    	return Stream.concat(NN.outputLayer.stream(), Stream.concat(NN.inputLayer.stream(), NN.hiddenLayers.stream()));
    }
    
    private static ArrayList<NN.Node> nodesInLayer(int layer){
    	return new ArrayList<NN.Node>(getNodes().filter(n -> n.layer == layer).collect(Collectors.toList()));
    }
    private static NN.Weight weightBetween(NN.Node from, NN.Node to){
    	return NN.weights.stream().filter(w -> w.startNode == from && w.endNode == to).findAny().get();
    }
    private static NN.Bias biasAt(NN.Node node){
    	return NN.biases.stream().filter(b -> b.parent == node).findAny().get();
    }
    
    public static double getNNError(InputAndExpectedOutput trainingSample) {
    	return getNNError(trainingSample.inputs, trainingSample.outputs, forwardProp(trainingSample.inputs));
    }
    public static double getNNError(ArrayList<Double> inputs, ArrayList<Double> expectedValues) {
    	return getNNError(inputs, expectedValues, forwardProp(inputs));
    }
    private static double getNNError(ArrayList<Double> inputs, ArrayList<Double> expectedValues, ArrayList<Double> actualValues) {
    	double totalError = 0;
    	for(int i = 0; i < expectedValues.size(); i++)
    		totalError += Math.pow(expectedValues.get(i) - actualValues.get(i), 2);
    	return totalError;
    }
    public static double getNNError(InputsAndExpectedOutputs trainingData) {
    	ArrayList<InputAndExpectedOutput> data = trainingData.trainingData;
    	double totalError = 0;
    	for(int i = 0; i < data.size(); i++)
    		totalError += Math.pow(getNNError(data.get(i)), 2);
    	return totalError;
    }
    
    public static ArrayList<Double> forwardProp(ArrayList<Double> inputs){
    	ArrayList<Double> prevBins = inputs;
    	
    	int layer = 0;
    	while(nodesInLayer(++layer).size() != 0) {
    		ArrayList<NN.Node> prevLayer = nodesInLayer(layer-1);
    		ArrayList<NN.Node> thisLayer = nodesInLayer(layer);
    		
    		ArrayList<Double> bins = new ArrayList<Double>();
    		for(NN.Node node : thisLayer) {
    			double postActivationValue = 0d;
    			for(NN.Node prevNode : prevLayer) {
    				postActivationValue += prevBins.get(prevNode.nodeIndex) * weightBetween(prevNode, node).value;
    			}
    			postActivationValue += biasAt(node).value;
    			
    			if(nodesInLayer(layer+1).size() != 0)
    				postActivationValue = NN.activationFunction.apply(postActivationValue);
    			
    			bins.add(postActivationValue);
    		}
    		
    		prevBins = bins;
    	}
    	return prevBins;
    }
    
    public static Double preValue(ArrayList<Double> inputs, NN.Node node){
    	ArrayList<Double> prevBins = inputs;
    	
    	int layer = 0;
    	while(nodesInLayer(++layer).size() != 0) {
    		ArrayList<NN.Node> prevLayer = nodesInLayer(layer-1);
    		ArrayList<NN.Node> thisLayer = nodesInLayer(layer);
    		
    		ArrayList<Double> bins = new ArrayList<Double>();
    		for(NN.Node n : thisLayer) {
    			double postActivationValue = 0d;
    			for(NN.Node prevNode : prevLayer) {
    				postActivationValue += prevBins.get(prevNode.nodeIndex) * weightBetween(prevNode, n).value;
    			}
    			postActivationValue += biasAt(n).value;
    			
    			if(n == node)
    				return postActivationValue;
    			
    			if(nodesInLayer(layer+1).size() != 0)
    				postActivationValue = NN.activationFunction.apply(postActivationValue);
    			
    			bins.add(postActivationValue);
    		}
    		
    		prevBins = bins;
    	}
    	throw new RuntimeException("Node not found");
    }
    public static Double postValue(ArrayList<Double> inputs, NN.Node node){
    	return NN.activationFunction.apply(preValue(inputs, node));
    }
    
    private static Stream<NN.NNParameter> getNNParameters(){
    	return Stream.concat(NN.weights.stream(), NN.biases.stream());
    }
    static class TrainingResult{
    	NN.NNParameter parameterToChange;
    	double newValue;
    	double newResultingError;
    }
    
    public static void gradientDescentTraining(InputsAndExpectedOutputs trainingData, Runnable printNN) {
    	while(true) {
    		printNN.run();
    		double learningRate = 0.1;
	    	Set<Entry<NNParameter, Double>> set = getNNParameters().collect(Collectors.toMap(val -> val, val -> val.getDerivative(trainingData))).entrySet();
	    	HashMap<NNParameter, Double> originalValues = new HashMap<NNParameter, Double>();
	    	set.forEach(entry -> originalValues.put(entry.getKey(), entry.getKey().value));
	    	
	    	//forwardProp(trainingData.trainingData.get(0).inputs)
	    	double newError, prevError = getNNError(trainingData);
	    	
	    	//Attempt an iteration
	    	double lr1 = learningRate;
	    	set.forEach(entry -> entry.getKey().value -= entry.getValue() * lr1);
	    	
	    	while(getNNError(trainingData) > prevError) {
	    		
	    		//Reset to the original values
	    		originalValues.forEach((nnParam, val) -> nnParam.value = val);
		    	learningRate /=2d;
		    	//System.out.println("New Learning Rate: " + learningRate);
		    	
		    	//Attempt the iteration again
	    		double lr3 = learningRate;
		    	set.forEach(entry -> entry.getKey().value -= entry.getValue() * lr3);
	    	}
	    	
	    	newError = getNNError(trainingData);
	    	System.out.println("Error: " + newError);
	    	System.out.println("Testing [1.0,0.5] -> [7, 20] actual (" + forwardProp(trainingData.trainingData.get(0).inputs) + ")");
	    	System.out.println("Testing [0.3,0.7] -> [101, 99] actual (" + forwardProp(trainingData.trainingData.get(1).inputs) + ")");
	    	System.out.println("Testing [0.1,0.1] -> [69, 69] actual (" + forwardProp(trainingData.trainingData.get(2).inputs) + ")");
	    	
	    	if(newError < Math.pow(10, -20))
	    		return;
    	}
    }
    static class Holder<T>{
    	T held;
    }
    private static double getBiasDerivative(InputsAndExpectedOutputs trainingData, NN.Bias bias) {
    	int layerToStopAt = bias.parent.layer;
    	
    	Holder<BiFunction<Integer, NN.Node, Double>> midPartSupplier = new Holder<>();
    	midPartSupplier.held = (sampleIndex, currentNeuron) -> {
    		ArrayList<Double> currentSampleInputs = trainingData.trainingData.get(sampleIndex).inputs;
    		if(currentNeuron.layer == layerToStopAt)
    			return bias.parent == currentNeuron? 1d : 0d;
    		
    		return getDerivativeMiddlePart(currentSampleInputs, sampleIndex, currentNeuron, midPartSupplier.held);
    	};
    	
    	return derivativeFirstPart(trainingData, midPartSupplier.held);
    }
    private static double getWeightDerivative(InputsAndExpectedOutputs trainingData, NN.Weight weight) {
    	int layerToStopAt = weight.endNode.layer;
    	
    	Holder<BiFunction<Integer, NN.Node, Double>> midPartSupplier = new Holder<>();
    	midPartSupplier.held = (sampleIndex, currentNeuron) -> {
    		ArrayList<Double> currentSampleInputs = trainingData.trainingData.get(sampleIndex).inputs;
    		if(currentNeuron.layer == layerToStopAt) {
    			/*
    			return NN.activationFunction.apply(preValue(currentSampleInputs, weight.startNode));
    			*/
    			if(weight.endNode == currentNeuron) {
    				double sum = 0;
    				for(NN.Node node : nodesInLayer(currentNeuron.layer-1)) {
    					if(weight.startNode == node) {
	    					if(currentNeuron.layer-1 == 0)
	    						sum += currentSampleInputs.get(node.nodeIndex);
	    					else
	    						sum += postValue(currentSampleInputs, node);
    					}
    				}
    				return sum;
    			}
    			
    			return 0d;
    		}
    		
    		return getDerivativeMiddlePart(currentSampleInputs, sampleIndex, currentNeuron, midPartSupplier.held);
    	};
    	
    	return derivativeFirstPart(trainingData, midPartSupplier.held);
    }
    private static double getDerivativeMiddlePart(ArrayList<Double> sampleInputs, int currentSampleIndex, NN.Node currentNode, BiFunction<Integer, NN.Node, Double> nextDerivativePart) {
    	double sum = 0;
    	ArrayList<NN.Node> prevLayerNodes = nodesInLayer(currentNode.layer-1);
    	for(NN.Node prevNode : prevLayerNodes) {
    		sum += NN.activationFunctionPrime.apply(preValue(sampleInputs, prevNode))*
    				weightBetween(prevNode, currentNode).value * 
    				nextDerivativePart.apply(currentSampleIndex, prevNode);
    	}
    	return sum;
    }
    private static double derivativeFirstPart(InputsAndExpectedOutputs trainingData, BiFunction<Integer, NN.Node, Double> nextDerivativePart) {
    	double result = 0;
    	for(int sampleIndex = 0; sampleIndex < trainingData.trainingData.size(); sampleIndex++) {
    		InputAndExpectedOutput sampleTrainingEntry = trainingData.trainingData.get(sampleIndex);
    		ArrayList<Double> currentOutput = forwardProp(sampleTrainingEntry.inputs);
    		double eachOutput = 0;
    		for(int outputNeuronIndex = 0; outputNeuronIndex < sampleTrainingEntry.outputs.size(); outputNeuronIndex++) {
    			eachOutput += 
    				(sampleTrainingEntry.outputs.get(outputNeuronIndex) - currentOutput.get(outputNeuronIndex)) *
    				nextDerivativePart.apply(sampleIndex, NN.outputLayer.get(outputNeuronIndex));
    		}
    		
    		result += getNNError(sampleTrainingEntry) * eachOutput;
    	}
    	
    	return -2 * result;
    }
    
    static class NN{
    	static ArrayList<InputLayerNode> inputLayer = new ArrayList<InputLayerNode>();
    	static ArrayList<HiddenLayerNode> hiddenLayers = new ArrayList<HiddenLayerNode>();
    	static ArrayList<OutputLayerNode> outputLayer = new ArrayList<OutputLayerNode>();

    	static ArrayList<Weight> weights = new ArrayList<Weight>();
    	static ArrayList<Bias> biases = new ArrayList<Bias>();
    	
    	static Function<Double, Double> activationFunction = (val) -> 1d/(1d + Math.pow(Math.E,-val));
    	static Function<Double, Double> activationFunctionPrime = (val) -> activationFunction.apply(val) * (1d-activationFunction.apply(val));
    	
    	public static void initWeights(BiFunction<Integer, Integer, Double> initialisationFunction) {
    		weights.forEach(weight -> weight.value = initialisationFunction.apply(
    				nodesInLayer(weight.startNode.layer).size(),
    				nodesInLayer(Math.min(weight.endNode.layer+1, outputLayer.get(0).layer)).size()
    			));
    	}
    	public static void initBiases(BiFunction<Integer, Integer, Double> initialisationFunction) {
    		biases.forEach(bias -> bias.value = initialisationFunction.apply(
    				nodesInLayer(bias.parent.layer-1).size(),
    				nodesInLayer(Math.min(bias.parent.layer+1, outputLayer.get(0).layer)).size()
    			));
    	}
    	
    	private static class Node{
    		int layer;
    		int nodeIndex;
    		Node(int layer, int nodeIndex){
    			this.layer = layer; this.nodeIndex = nodeIndex;
    		}
    	}
    	static class InputLayerNode extends Node{
			InputLayerNode(int nodeIndex) {
				super(0, nodeIndex);
				inputLayer.add(this);
			}
    	}
    	static class HiddenLayerNode extends Node{
    		HiddenLayerNode(int layer, int nodeIndex) {
				super(layer, nodeIndex);
				hiddenLayers.add(this);
			}
    	}
    	static class OutputLayerNode extends Node{
    		OutputLayerNode(int layer, int nodeIndex) {
				super(layer, nodeIndex);
				outputLayer.add(this);
			}
    	}
    	
    	static abstract class NNParameter{
    		double value = 0;
    		abstract double getDerivative(InputsAndExpectedOutputs trainingData);
    	}
    	static class Weight extends NN.NNParameter{
			Node startNode;
    		Node endNode;
    		{
    			weights.add(this);
    		}
    		public Weight(Node from, Node to) {
    			startNode = from;
    			endNode = to;
			}
			@Override
			double getDerivative(InputsAndExpectedOutputs trainingData) {
				return getWeightDerivative(trainingData, this);
			}
    	}
    	static class Bias extends NN.NNParameter{
    		Node parent;
    		{
    			biases.add(this);
    		}
    		public Bias(OutputLayerNode parent) {
    			this.parent = parent;
			}
    		public Bias(HiddenLayerNode parent) {
    			this.parent = parent;
			}
			@Override
			double getDerivative(InputsAndExpectedOutputs trainingData) {
				return getBiasDerivative(trainingData, this);
			}
    	}
    }
}
